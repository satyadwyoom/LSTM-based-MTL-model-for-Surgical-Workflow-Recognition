{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "several-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/pbs.2777367.pbsha.ib.sockeye/matplotlib-8w8mgo88 because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "#### Load Libraries ####\n",
    "from __future__ import print_function, division\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.resnet import resnet50\n",
    "import torch.backends.cudnn as cudnn\n",
    "import cv2  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#### PATH TO GROUND TRUTH INPUT AND OUTPUT FOLDERS ###\n",
    "\n",
    "path_of_video = './Video'\n",
    "path_of_kinm = './Kinematic'\n",
    "path_of_output = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT DATA INTO TRAIN AND TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "iraqi-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video = [i for i in sorted(os.listdir(path_of_video)) if int(i.split('.')[0]) > 100]\n",
    "test_kin = [i for i in sorted(os.listdir(path_of_kinm)) if int(i.split('.')[0]) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sporting-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeLayer(torch.nn.Module):\n",
    "    \"\"\"Standardize the channels of a batch of images by subtracting the dataset mean\n",
    "      and dividing by the dataset standard deviation.\n",
    "\n",
    "      In order to certify radii in original coordinates rather than standardized coordinates, we\n",
    "      add the Gaussian noise _before_ standardizing, which is why we have standardization be the first\n",
    "      layer of the classifier rather than as a part of preprocessing as is typical.\n",
    "      \"\"\"\n",
    "\n",
    "    def __init__(self, means, sds):\n",
    "        \"\"\"\n",
    "        :param means: the channel means\n",
    "        :param sds: the channel standard deviations\n",
    "        \"\"\"\n",
    "        super(NormalizeLayer, self).__init__()\n",
    "        self.means = torch.tensor(means).cuda()\n",
    "        self.sds = torch.tensor(sds).cuda()\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        (batch_size, num_channels, height, width) = input.shape\n",
    "        means = self.means.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n",
    "        sds = self.sds.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n",
    "        return (input - means)/sds\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#### LOAD RESNET50 PRETRAINED IMAGENET WEIGHTS \n",
    "\n",
    "resnet50_imagenet = torch.load('./trained_models/ex.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PETRAW MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convenient-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "class petraw_model(nn.Module):\n",
    "    def __init__(self, imagenet_extractor=None, input_size=28, output_size=None, time_depth=0):\n",
    "        super(petraw_model, self).__init__()\n",
    "        \n",
    "        self.imagenet_extractor = imagenet_extractor\n",
    "        \n",
    "        \n",
    "        self.lin_kin = nn.Linear(input_size, 64)\n",
    "        self.linear = nn.Linear(64, 32)\n",
    "        hidden_size = 32\n",
    "        \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        ### Video Layers\n",
    "        self.fc_ex1 = nn.Linear(2048, 256)\n",
    "        self.fc_ex2 = nn.Linear(256, 64)\n",
    "        ex_size = 64\n",
    "\n",
    "        \n",
    "\n",
    "        ## Video Frame-> Imagenet_extractor-> 2048 -> Fc_ex1 -> Fc_ex2 -> Fc_ex3 -> Ourt 1\n",
    "        \n",
    "        ### output-layers\n",
    "        self.fc_F = nn.Linear(hidden_size + ex_size, output_size)\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.drop3 = nn.Dropout(p=0.3)\n",
    "        self.drop4 = nn.Dropout(p=0.3)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, input_seq, input_image):\n",
    "        \n",
    "        lin_out = self.lin_kin(input_seq)\n",
    "        lin_out = self.relu1(lin_out)\n",
    "        lin_out = self.drop1(lin_out)\n",
    "        lin_out = self.linear(lin_out)\n",
    "        lin_out = self.relu2(lin_out)\n",
    "        lin_out = self.drop2(lin_out)\n",
    "            \n",
    "        if input_image is not None:\n",
    "            x = self.fc_ex1(input_image)\n",
    "            x = self.relu3(x)\n",
    "            x = self.drop3(x)\n",
    "            x = self.fc_ex2(x)\n",
    "            x = self.relu4(x)\n",
    "            x = self.drop4(x)\n",
    "            a = torch.cat((lin_out, x), dim=1)\n",
    "            y_f = self.fc_F(a)\n",
    "        else:\n",
    "            a = self.drop1(lin_out)\n",
    "            y_f = self.fc_F(a) \n",
    "\n",
    "        return y_f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD TRAINED PETRAW MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automatic-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './trained_models/test.pth.tar'\n",
    "model_list = torch.load(model_path)\n",
    "\n",
    "\n",
    "\n",
    "#### OUTPUT MAPPING DICTIONARY \n",
    "\n",
    "output_dict_list = [{0 :'Idle', 1: 'Transfer Left to Right', 2: 'Transfer Right to Left'},\n",
    "                    {0: 'Idle', 1: 'Block 1 L2R', 2: 'Block 2 L2R', 3: 'Block 3 L2R', 4: 'Block 4 L2R', 5: 'Block 5 L2R', 6: 'Block 6 L2R', 7: 'Block 1 R2L', 8: 'Block 2 R2L', 9: 'Block 3 R2L', 10:'Block 4 R2L', 11:'Block 5 R2L', 12:'Block 6 R2L'},\n",
    "                    {0: 'Idle', 1:'Catch', 2:'Extract', 3: 'Hold', 4: 'Drop', 5: 'Touch', 6: 'Insert'},\n",
    "                    {0: 'Idle', 1:'Catch', 2:'Extract', 3: 'Hold', 4: 'Drop', 5: 'Touch', 6: 'Insert'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE SURGICAL WORKFLOW PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "destroyed-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_transform = T.Compose([T.Resize((224, 224)),\n",
    "                         T.ToTensor()])\n",
    "    \n",
    "def read_video_kin_path(v_path, k_path, model_list, output_dict_list):\n",
    "\n",
    "    output_list = [[],[],[],[]]\n",
    "    kin_input = pd.read_csv(k_path, delimiter = '\\t', index_col = 0)\n",
    "    cap= cv2.VideoCapture(v_path)\n",
    "    i=0\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frame = f_transform(frame).unsqueeze(0).to(device)\n",
    "        frame = resnet50_imagenet(frame).detach()\n",
    "        kmatic = torch.from_numpy(kin_input.iloc[i].to_numpy()).unsqueeze(0).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            for m in range(len(model_list)):\n",
    "                o_dict = output_dict_list[m]\n",
    "                model, _ = model_list[m]\n",
    "                model.to(device).eval()\n",
    "                out = model(kmatic, frame)\n",
    "                predicted_class = nn.Softmax(dim=1)(out).max(dim=1)[1]\n",
    "                predicted_value = [o_dict[k.cpu().item()] for k in predicted_class]\n",
    "                output_list[m].extend(predicted_value)                   \n",
    "        i+=1\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (v, k) in zip(test_video, test_kin):\n",
    "    v_path = os.path.join(path_of_video, v)\n",
    "    k_path = os.path.join(path_of_kinm, k)\n",
    "    predicted_output_val = read_video_kin_path(v_path, k_path, model_list, output_dict_list)\n",
    "    predicted_output_dict = {'Frame': [i for i in range(len(predicted_output_val[0]))],\n",
    "                             'Phase': predicted_output_val[0],\n",
    "                             'Step': predicted_output_val[1],\n",
    "                             'Verb_Left': predicted_output_val[2],\n",
    "                             'Verb_Right': predicted_output_val[3]}\n",
    "    predicted_output_df = pd.DataFrame.from_dict(predicted_output_dict)\n",
    "    predicted_output_df.to_csv(os.path.join(path_of_output, v.split('.')[0]+'.txt'), index=False, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me_jpy",
   "language": "python",
   "name": "me_jpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
