{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JJ9-fITgaoCh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/pbs.2739780.pbsha.ib.sockeye/matplotlib-k3p15z47 because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim, as_tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.resnet import resnet18, resnet34, resnet50\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqUy0O_Rp84q",
    "outputId": "29c99f57-ead9-4bad-a266-1985f7d4a62c"
   },
   "outputs": [],
   "source": [
    "# kin_data = pd.read_csv('dataset_path/inputs.csv', index_col = 0)\n",
    "# output_data  = pd.read_csv('dataset_path/outputs.csv', index_col = 0)\n",
    "frame_data = pd.read_csv('dataset_path/Frames.csv', index_col = 0) \n",
    "# seg_data = pd.read_csv('dataset_path/Segmentation.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYjPqIfnqqif",
    "outputId": "4664a83d-6096-45fc-f299-f4edd2d31103"
   },
   "outputs": [],
   "source": [
    "# Phase_dict = {output_data['Phase'].unique()[i]:i for i in range(len(output_data['Phase'].unique()))}\n",
    "# Step_dict = {output_data['Step'].unique()[i]:i for i in range(len(output_data['Step'].unique()))}\n",
    "# Verb_Left_dict = {output_data['Verb_Left'].unique()[i]:i for i in range(len(output_data['Verb_Left'].unique()))}\n",
    "# Verb_Right_dict = Verb_Left_dict\n",
    "\n",
    "# print('Output Mapping Dictionary: \\n')\n",
    "# print('Phase Dict:', Phase_dict)\n",
    "# print('Step Dict:', Step_dict)\n",
    "# print('Verb Left Dict:', Verb_Left_dict)\n",
    "# print('Verb Right Dict:', Verb_Right_dict)\n",
    "\n",
    "\n",
    "# output_data['Phase'] = output_data['Phase'].map(Phase_dict)\n",
    "# output_data['Step'] = output_data['Step'].map(Step_dict)\n",
    "# output_data['Verb_Left'] = output_data['Verb_Left'].map(Verb_Left_dict)\n",
    "# output_data['Verb_Right'] = output_data['Verb_Right'].map(Verb_Right_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "NAija1xRqS1T",
    "outputId": "239327dd-eca6-4593-b0a1-be2420fb9bb7"
   },
   "outputs": [],
   "source": [
    "# ##### Input Features #####\n",
    "# kin_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "9EcDDWPzqf8j",
    "outputId": "afc20d32-dfcd-49a3-d515-96d7fc83196c"
   },
   "outputs": [],
   "source": [
    "# ##### Output Features #####\n",
    "# output_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### frames #####\n",
    "# frame_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Segmentation map #####\n",
    "# seg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5SZzWRSTz6tl"
   },
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    \"\"\"Randomly Crop the frames in a clip.\"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "              output_size (tuple or int): Desired output size. If int, square crop\n",
    "              is made.\n",
    "        \"\"\"\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        h, w = clip.size()[2:]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        clip = clip[:, :, top : top + new_h, left : left + new_w]\n",
    "\n",
    "        return clip\n",
    "\n",
    "\n",
    "class GeneralVideoDataset(Dataset):\n",
    "    \"\"\"Dataset Class for Loading Video\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seg=None,\n",
    "        frame=None,\n",
    "        kin=None,\n",
    "        output=None,\n",
    "        time_depth=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clips_list_file (string): Path to the clipsList file with labels.\n",
    "            root_dir (string): Directory with all the videoes.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            channels: Number of channels of frames\n",
    "            time_depth: Number of frames to be loaded in a sample\n",
    "            x_size, y_size: Dimensions of the frames\n",
    "            mean: Mean value of the training set videos over each channel\n",
    "        \"\"\"\n",
    "        self.seg_df = seg\n",
    "        self.frame_df = frame\n",
    "        self.kin_df = kin\n",
    "        self.output_df = output\n",
    "        \n",
    "        self.transform = T.Compose([T.Resize((224, 224)),\n",
    "                                    T.ToTensor()])\n",
    "        \n",
    "#         assert len(self.seg_df) == len(self.frame_df) == len(self.kin_df) == len(self.output_df)\n",
    "        # self.time_depth = time_depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_df)\n",
    "\n",
    "    def read_image(self, df, idx):\n",
    "        # Load the image file\n",
    "        image = Image.open(df.iloc[idx]['file_path'])\n",
    "        x = self.transform(image)\n",
    "#         x.unsqueeze_(0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        video_frame = self.read_image(self.frame_df, idx)\n",
    "#         seg_map = self.read_image(self.seg_df, idx)\n",
    "\n",
    "#         kinematic = torch.from_numpy(self.kin_df.iloc[idx].to_numpy()).float().unsqueeze_(0)\n",
    "#         output = torch.from_numpy(self.output_df.iloc[idx].to_numpy())\n",
    "\n",
    "        sample = {\n",
    "            \"video\": video_frame,\n",
    "#             \"segmenation\": seg_map,\n",
    "#             \"Kinematic\": kinematic,\n",
    "#             \"output\": output,\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet 50 loaded and layers freezed\n"
     ]
    }
   ],
   "source": [
    "class NormalizeLayer(torch.nn.Module):\n",
    "    \"\"\"Standardize the channels of a batch of images by subtracting the dataset mean\n",
    "      and dividing by the dataset standard deviation.\n",
    "\n",
    "      In order to certify radii in original coordinates rather than standardized coordinates, we\n",
    "      add the Gaussian noise _before_ standardizing, which is why we have standardization be the first\n",
    "      layer of the classifier rather than as a part of preprocessing as is typical.\n",
    "      \"\"\"\n",
    "\n",
    "    def __init__(self, means, sds):\n",
    "        \"\"\"\n",
    "        :param means: the channel means\n",
    "        :param sds: the channel standard deviations\n",
    "        \"\"\"\n",
    "        super(NormalizeLayer, self).__init__()\n",
    "        self.means = torch.tensor(means).cuda()\n",
    "        self.sds = torch.tensor(sds).cuda()\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        (batch_size, num_channels, height, width) = input.shape\n",
    "        means = self.means.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n",
    "        sds = self.sds.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n",
    "        return (input - means)/sds\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    \n",
    "norm_layer = NormalizeLayer(means=[0.485, 0.456, 0.406],\n",
    "                            sds=[0.229, 0.224, 0.225])\n",
    "\n",
    "resnet50_imagenet = resnet50(pretrained=False).cuda()\n",
    "resnet50_img_weight_path = 'pretrained_model/checkpoint.pth.tar'\n",
    "checkpoint = torch.load(resnet50_img_weight_path)\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if k[:1]!=str(0):\n",
    "        name = k[9:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "resnet50_imagenet.load_state_dict(new_state_dict)\n",
    "resnet50_imagenet.fc = Identity()\n",
    "resnet50_imagenet = torch.nn.Sequential(norm_layer, resnet50_imagenet)\n",
    "\n",
    "for param50 in resnet50_imagenet.parameters():\n",
    "    param50.requires_grad = False\n",
    "    \n",
    "print('resnet 50 loaded and layers freezed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DWeY-E6tenN6"
   },
   "outputs": [],
   "source": [
    "bs = 128\n",
    "train_data = GeneralVideoDataset(frame=frame_data)\n",
    "loader = DataLoader(train_data, bs, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2958"
     ]
    }
   ],
   "source": [
    "frame_data_list = []\n",
    "for batch_idx, X in enumerate(loader):\n",
    "    frames = X['video']\n",
    "    output_encoded = resnet50_imagenet(frames.cuda()).cpu().detach()\n",
    "    frame_data_list.append(output_encoded)\n",
    "    print('\\rBatch: {}'.format(batch_idx+1), flush=True,  end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_array = torch.cat(frame_data_list, dim=0).numpy()\n",
    "np.save('./dataset_path/video_frames.npy', frame_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "petraw_challenge_kinematic_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "me_jpy",
   "language": "python",
   "name": "me_jpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
