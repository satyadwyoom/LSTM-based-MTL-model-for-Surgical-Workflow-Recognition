{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Simple PyTorch Video Dataset Class for loading videos using PyTorch\n",
    "Dataloader. This Dataset assumes that video files are Preprocessed\n",
    " by being trimmed over time and resizing the frames.\n",
    "If you find this code useful, please star the repository.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Randomly Crop the frames in a clip.\"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "              output_size (tuple or int): Desired output size. If int, square crop\n",
    "              is made.\n",
    "        \"\"\"\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        h, w = clip.size()[2:]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        clip = clip[:, :, top : top + new_h, left : left + new_w]\n",
    "\n",
    "        return clip\n",
    "\n",
    "\n",
    "class GeneralVideoDataset(Dataset):\n",
    "    \"\"\"Dataset Class for Loading Video\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        root_dir_video,\n",
    "        root_dir_seg,\n",
    "        root_dir_kin,\n",
    "        labels,\n",
    "        channels,\n",
    "        time_depth,\n",
    "        x_size,\n",
    "        y_size,\n",
    "        mean,\n",
    "        stddev,\n",
    "        transform=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir_video (string): Directory with all the video data.\n",
    "            root_dir_seg (string): Directory with all the segmentation data.\n",
    "            root_dir_kin (string): Directory with all the kinematics data.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            channels: Number of channels of frames\n",
    "            time_depth: Number of frames to be loaded in a sample\n",
    "            x_size, y_size: Dimensions of the frames\n",
    "            mean: Mean value of the training set videos over each channel\n",
    "        \"\"\"\n",
    "        \n",
    "        root_dir_video = os.path.join(data_path, root_dir_video)\n",
    "        root_dir_seg = os.path.join(data_path, root_dir_seg)\n",
    "        root_dir_kin = os.path.join(data_path, root_dir_kin)\n",
    "        labels = os.path.join(data_path, labels)\n",
    "        \n",
    "        self.clips_list_video = sorted(os.listdir(root_dir_video))\n",
    "        self.clips_list_seg = sorted(os.listdir(root_dir_seg))\n",
    "        self.clips_list_kin = sorted(os.listdir(root_dir_kin))\n",
    "        self.labels = sorted(os.listdir(labels))\n",
    "        \n",
    "        self.root_dir_video = root_dir_video\n",
    "        self.root_dir_seg = root_dir_seg\n",
    "        self.root_dir_kin = root_dir_kin\n",
    "        self.channels = channels\n",
    "        self.time_depth = time_depth\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clips_list_video)\n",
    "\n",
    "    def read_video(self, video_file):\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        frames = torch.FloatTensor(\n",
    "            self.channels, self.x_size, self.y_size\n",
    "        )\n",
    "        failed_clip = False\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = torch.from_numpy(frame)\n",
    "            # HWC2CHW\n",
    "            frame = frame.permute(2, 0, 1)\n",
    "            frames[:, :, :] = frame\n",
    "\n",
    "        else:\n",
    "            print(\"Skipped!\")\n",
    "            failed_clip = True\n",
    "            break\n",
    "\n",
    "        for idx in range(len(self.mean)):\n",
    "            frames[idx] = (frames[idx] - self.mean[idx]) / self.stddev[idx]\n",
    "\n",
    "        return frames, failed_clip\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        video_file = os.path.join(self.root_dir, self.clips_list[idx][0])\n",
    "        clip, failed_clip = self.read_video(video_file)\n",
    "        if self.transform:\n",
    "            clip = self.transform(clip)\n",
    "        sample = {\n",
    "            \"clip\": clip,\n",
    "            \"label\": self.clips_list[idx][1],\n",
    "            \"failedClip\": failed_clip,\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me_jpy",
   "language": "python",
   "name": "me_jpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
